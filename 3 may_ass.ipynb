{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106d66a-0b53-4d7b-8cac-8c6a776eccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the role of feature selection in anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da82d8a-10ba-4b70-9c67-40aed53e4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "The role of feature selection in anomaly detection is to identify and select the most relevant and informative features from the dataset that can effectively distinguish \n",
    "between normal and anomalous instances. Feature selection plays a crucial role in anomaly detection for the following \n",
    "reasons:\n",
    "\n",
    "Dimensionality Reduction: Anomaly detection often deals with high-dimensional data where the number of features or attributes is large. Feature selection helps reduce the \n",
    "dimensionality of the data by selecting a subset of features that contribute the most to differentiating between normal and anomalous instances. This reduces computational\n",
    "complexity and can improve the efficiency and effectiveness of anomaly detection algorithms.\n",
    "\n",
    "Noise Reduction: Some features in the dataset may contain noise or irrelevant information that can hinder the detection of anomalies. Feature selection helps eliminate or\n",
    "reduce the impact of noisy features, allowing anomaly detection algorithms to focus on the most informative features. Removing irrelevant features can also improve the\n",
    "accuracy and interpretability of the anomaly detection results.\n",
    "\n",
    "Improved Detection Performance: By selecting the most relevant features, feature selection can enhance the performance of anomaly detection algorithms. Relevant features\n",
    "capture the underlying patterns and characteristics of normal and anomalous instances, leading to better discrimination and more accurate detection of anomalies. Selecting \n",
    "informative features can help in capturing the unique characteristics of anomalies and avoiding false positives or false negatives.\n",
    "\n",
    "Interpretability and Explainability: Feature selection can lead to more interpretable and explainable anomaly detection models. By focusing on a subset of features, the \n",
    "selected features can be easily understood and interpreted, allowing domain experts to gain insights into the factors influencing the occurrence of anomalies.\n",
    "\n",
    "Computational Efficiency: Feature selection reduces the computational burden by reducing the number of features that need to be processed and analyzed. This is particularly\n",
    "important when dealing with large-scale datasets or real-time anomaly detection applications where efficiency is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d721f1fa-9dfc-4645-96bd-e074bbb8f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are some common evaluation metrics for anomaly detection algorithms and how are they\n",
    "computed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a0e21-ec82-4dcb-a052-2e61dd0dc91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several evaluation metrics commonly used to assess the performance of anomaly detection algorithms. Here are some of the most common evaluation metrics and how\n",
    "they are computed:\n",
    "\n",
    "Accuracy: Accuracy measures the overall correctness of the anomaly detection algorithm in classifying instances as normal or anomalous. It is computed as the ratio of\n",
    "correctly classified instances (both true positives and true negatives) to the total number of instances.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision: Precision measures the proportion of correctly identified anomalies among all instances classified as anomalies. It is computed as the ratio of true positives \n",
    "to the sum of true positives and false positives.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall (Sensitivity or True Positive Rate): Recall measures the ability of the algorithm to identify all positive instances (anomalies) correctly. It is computed as the\n",
    "ratio of true positives to the sum of true positives and false negatives.\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score: The F1 score combines precision and recall into a single metric, providing a balanced measure of the algorithm's performance. It is the harmonic mean of \n",
    "precision and recall and is computed as:\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Area Under the Receiver Operating Characteristic Curve (AUC-ROC): The AUC-ROC is a widely used metric for evaluating the performance of anomaly detection algorithms. \n",
    "It measures the algorithm's ability to discriminate between normal and anomalous instances across various threshold settings. The ROC curve plots the True Positive \n",
    "Rate (TPR) against the False \n",
    "Positive Rate (FPR) at different threshold values. The AUC-ROC is the area under this curve and ranges from 0 to 1, where a higher value indicates better performance.\n",
    "\n",
    "Area Under the Precision-Recall Curve (AUC-PR): The AUC-PR is another metric that evaluates the performance of anomaly detection algorithms based on the precision-recall\n",
    "trade-off. It measures the area under the curve obtained by plotting precision against recall at different threshold settings. Similar to AUC-ROC, a higher AUC-PR value \n",
    "indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa43452-7972-42d8-9ee8-8c0e2fc3a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is DBSCAN and how does it work for clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997c4d6-10d0-46b2-887d-fba2cd39bb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a popular density-based clustering algorithm. It is designed to discover clusters of arbitrary \n",
    "shape in a dataset and can identify outliers as noise points. DBSCAN works based on the idea that clusters are areas of high density separated by areas of low density.\n",
    "\n",
    "Here's how DBSCAN works for clustering:\n",
    "\n",
    "Density-based Neighbors: DBSCAN defines core points, border points, and noise points based on the density of points in their vicinity. A core point is a point that has a \n",
    "sufficient number of neighboring points within a specified distance (epsilon) called the epsilon neighborhood. Border points have fewer neighbors than the core points but\n",
    "are within the epsilon neighborhood of a core point. Noise points, also called outliers, have very few or no neighboring points within the epsilon neighborhood.\n",
    "\n",
    "Cluster Formation: The DBSCAN algorithm starts by randomly selecting an unvisited point and expanding the cluster around it. It identifies all the core points within the\n",
    "epsilon neighborhood of the selected point and recursively expands the cluster to include all density-reachable points. Density-reachable points are those that can be\n",
    "reached by a chain of core points. This process continues until no more density-reachable points can be found.\n",
    "\n",
    "Border Points: DBSCAN assigns border points to the cluster of their corresponding core points. If a border point is within the epsilon neighborhood of multiple core points,\n",
    "it is assigned to one of the clusters but does not create a new cluster itself.\n",
    "\n",
    "Noise Points: Noise points are not assigned to any cluster and are labeled as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afc4ad-1a0f-4626-9f8a-a8bb8dde19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the epsilon parameter affect the performance of DBSCAN in detecting anomalies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e19ff-ff84-484f-a79d-00bac4b0871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The epsilon parameter in DBSCAN defines the maximum distance between two points for them to be considered neighbors. It plays a crucial role in the performance of DBSCAN\n",
    "in detecting anomalies. Here's how the epsilon parameter affects the performance:\n",
    "\n",
    "Sensitivity to Density: The epsilon parameter determines the neighborhood size for defining core points in DBSCAN. Smaller epsilon values result in tighter clusters with\n",
    "higher density requirements for core points. As a result, anomalies that lie in low-density regions may not be considered outliers if they have some neighboring points \n",
    "within the smaller epsilon neighborhood.\n",
    "Conversely, larger epsilon values make it easier for points to be considered outliers, as fewer neighboring points are required. Therefore, the choice of epsilon should\n",
    "consider the density characteristics of the dataset and the desired sensitivity to outliers.\n",
    "\n",
    "Anomaly Detection Threshold: The epsilon parameter serves as a threshold for distinguishing normal points from anomalies. Points that have very few or no neighboring \n",
    "points within the epsilon distance are considered outliers or noise points. By adjusting the epsilon parameter, the threshold for anomaly detection can be fine-tuned. \n",
    "Smaller epsilon values result in stricter outlier detection, allowing only isolated points to be considered anomalies. On the other hand, larger epsilon values increase \n",
    "the chance of including points in dense regions as part of clusters, potentially reducing the sensitivity to anomalies.\n",
    "\n",
    "Trade-off between Precision and Recall: The choice of epsilon in DBSCAN involves a trade-off between precision (correctly identifying anomalies) and recall (detecting \n",
    "all anomalies). Smaller epsilon values may lead to higher precision but lower recall, as only isolated anomalies are detected. Conversely, larger epsilon values may \n",
    "increase the recall but can also introduce false positives by including normal points near the boundaries of clusters.\n",
    "\n",
    "Dataset Characteristics: The impact of the epsilon parameter on anomaly detection performance depends on the characteristics of the dataset, such as the density and\n",
    "distribution of anomalies. In datasets with varying densities or clusters of different sizes, it may be necessary to adapt the epsilon parameter accordingly to capture\n",
    "anomalies across the data's density landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62201d8-ced3-44ef-a5ca-c37b5e9275df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What are the differences between the core, border, and noise points in DBSCAN, and how do they relate\n",
    "to anomaly detection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34223f62-5c5e-4145-9640-931e6aa6b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), three types of points are identified: core points, border points, and noise points. These \n",
    "points play a crucial role in identifying clusters and detecting anomalies. Here's a breakdown of their differences and their relation to anomaly detection:\n",
    "\n",
    "Core Points: Core points are the central points within a cluster. They are defined by having a sufficient number of neighboring points within a specified distance \n",
    "(epsilon) called the epsilon neighborhood. In other words, a core point has at least \"MinPts\" number of points (including itself) within its epsilon neighborhood. \n",
    "Core points are considered representative of the cluster and form the foundation for cluster formation. They contribute to the density and structure of the clusters.\n",
    "Relation to Anomaly Detection: Core points are not typically considered anomalies themselves, as they are part of dense regions and contribute to cluster formation. \n",
    "However, anomalies may be present as outliers within the epsilon neighborhood of core points if they have a small number of neighboring points. These outliers may be \n",
    "classified as noise points.\n",
    "\n",
    "Border Points: Border points are the points that have fewer neighbors than the core points but are within the epsilon neighborhood of a core point. In other words, they\n",
    "are within the reachability distance of a core point. Border points are part of a cluster but are not as densely connected as core points. They lie on the boundaries of\n",
    "clusters and connect the clusters together.\n",
    "Relation to Anomaly Detection: Border points are not typically considered anomalies either, as they are part of clusters. However, anomalies that lie close to the boundaries\n",
    "of clusters may be classified as border points if they have some neighboring points within the epsilon neighborhood of a core point. The classification of anomalies as \n",
    "border points depends on their proximity to core points and the chosen epsilon value.\n",
    "\n",
    "Noise Points: Noise points, also known as outliers, are the points that have very few or no neighboring points within their epsilon neighborhood. They do not belong to \n",
    "any cluster and are not connected to other points. Noise points are often considered anomalies as they deviate significantly from the density patterns observed in the\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff545a0f-771e-4128-93bf-2dc704bc0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does DBSCAN detect anomalies and what are the key parameters involved in the process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fbb2e-2977-4633-a9a8-e29264cb05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is primarily designed for clustering, but it can also be used for anomaly detection by identifying \n",
    "points that do not fit within any cluster. Here's how DBSCAN detects anomalies and the key parameters involved in the process:\n",
    "\n",
    "Density-Based Approach: DBSCAN detects anomalies based on the density of points in the dataset. Anomalies are identified as points that have low-density neighborhoods \n",
    "or do not belong to any cluster. The idea is that anomalies often exhibit lower density or differ significantly from the underlying density patterns observed in the data.\n",
    "\n",
    "Key Parameters:\n",
    "a. Epsilon (ε): Also known as the radius parameter, epsilon defines the maximum distance between two points for them to be considered neighbors. It determines the size \n",
    "of the neighborhood around each point.\n",
    "b. Minimum points (MinPts): MinPts specifies the minimum number of neighboring points within the epsilon neighborhood for a point to be considered a core point. Core \n",
    "points are the central points within clusters, and they play a crucial role in the clustering process.\n",
    "\n",
    "Core Points and Reachability: DBSCAN starts by randomly selecting an unvisited point and expands the cluster around it. A core point is a point that has at least MinPts\n",
    "number of points (including itself) within its epsilon neighborhood. Core points are connected to each other, forming clusters. They are reached by a direct or indirect\n",
    "path of core points.\n",
    "\n",
    "Density-Reachable and Density-Connected: Density-reachable points are those that can be reached by a chain of core points, but they may not have enough neighboring points\n",
    "to be considered core points themselves. Density-connected points are a group of density-reachable points that share the same cluster. They are connected through a \n",
    "sequence of core points.\n",
    "\n",
    "Anomaly Detection:\n",
    "a. Noise Points: Points that have very few or no neighboring points within their epsilon neighborhood are considered noise points. These points do not belong to any \n",
    "cluster and are labeled as anomalies.\n",
    "b. Border Points: Border points have fewer neighbors than core points but are within the epsilon neighborhood of a core point. Anomalies that lie close to the boundaries\n",
    "of clusters may be classified as border points if they have some neighboring points within the epsilon neighborhood of a core point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed3890f-3d42-43f8-abb4-0f067323bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What is the make_circles package in scikit-learn used for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f40576-7bb3-4676-8bbd-6ee9f3c7d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "The make_circles package in scikit-learn is a utility function used to generate a synthetic dataset of concentric circles. It is primarily used for testing and \n",
    "illustrating algorithms that aim to solve classification or clustering problems with non-linear decision boundaries.\n",
    "\n",
    "The make_circles function allows you to generate a 2D dataset with two classes that form interlocking circles. The circles can be configured to have different levels \n",
    "of noise and separation between the classes. This synthetic dataset is useful for evaluating the performance of algorithms that \n",
    "are sensitive to non-linear relationships and can handle complex patterns.\n",
    "\n",
    "The make_circles function takes several parameters, including the number of samples, noise level, factor, and random state. By adjusting these parameters, you can control \n",
    "the characteristics of the generated dataset, such as the number of samples, the amount of noise present, and the tightness of the circles.\n",
    "\n",
    "Once generated, the make_circles dataset can be used to train and evaluate machine learning models for classification or clustering tasks. It provides a controlled\n",
    "environment for testing algorithms' ability to capture non-linear patterns and separate the classes formed by the concentric circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe8007-bc17-4787-81e1-6f7afbaf1ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are local outliers and global outliers, and how do they differ from each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ba916-f0e4-4aa1-b448-4157754f524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n the context of outlier detection, local outliers and global outliers refer to different types of anomalies within a dataset. Here's how they differ from each other:\n",
    "\n",
    "Local Outliers: Local outliers, also known as contextual outliers or point anomalies, are data points that deviate from the surrounding data points in a localized region.\n",
    "They exhibit unusual behavior compared to their immediate neighbors but may not be considered outliers when considering the dataset as a whole. Local outliers are identified \n",
    "by examining the local density or behavior of data points within a specific neighborhood or region. These anomalies are relevant\n",
    "within their local context but may not be considered outliers when considering the entire dataset.\n",
    "Example: In a temperature dataset, a local outlier could represent a sudden spike or drop in temperature that is significantly different from the neighboring data points\n",
    "but still falls within the range of temperatures observed in a specific region or time period.\n",
    "\n",
    "Global Outliers: Global outliers, also known as global anomalies or collective anomalies, are data points that deviate significantly from the overall distribution or \n",
    "pattern of the entire dataset. These outliers are rare events or observations that are anomalous when considering the dataset as a whole. Global outliers exhibit \n",
    "unusual behavior compared to the majority of data points and are often considered outliers across the entire dataset, regardless of their local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569d420-36ef-4f44-afc7-97fb459e26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How can local outliers be detected using the Local Outlier Factor (LOF) algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c539b-9414-4f60-a196-bf47628b493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Local Outlier Factor (LOF) algorithm is a popular method for detecting local outliers within a dataset. It quantifies the deviation of a data point's density compared to\n",
    "its neighboring points, allowing for the identification of points that exhibit unusual behavior within their local context. Here's how LOF detects local outliers:\n",
    "\n",
    "Determine the neighborhood: For each data point in the dataset, the LOF algorithm identifies its k-nearest neighbors based on a chosen distance metric. The value of k is a\n",
    "user-defined parameter that determines the size of the local neighborhood.\n",
    "\n",
    "Calculate local reachability distance: The local reachability distance of a data point is a measure of how far the point is from its neighbors. It is calculated as the \n",
    "inverse of the average distance between the point and its k-nearest neighbors. A lower average distance indicates a higher density, while a higher average distance \n",
    "indicates a lower density.\n",
    "\n",
    "Compute local outlier factor: The local outlier factor (LOF) is computed for each data point based on the local reachability distances of its neighbors. It represents \n",
    "the degree of outlierness of the point within its local context. A higher LOF value indicates that the point is more likely to be a local outlier.\n",
    "\n",
    "Compare LOF values: By comparing the LOF values of different data points, it is possible to identify local outliers. Points with LOF values significantly higher than \n",
    "the average LOF values of their neighbors are considered local outliers. These points exhibit a lower density compared to their neighbors, suggesting that they are \n",
    "dissimilar or anomalous within their local context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e1788-c83a-4516-af07-dd4cc5290c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. How can global outliers be detected using the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf87e0-cd8f-43b4-8653-1dd0c681e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Isolation Forest algorithm is a popular method for detecting global outliers within a dataset. It leverages the concept of isolation to identify data points that are\n",
    "significantly different from the \n",
    "majority of the data. Here's how the Isolation Forest algorithm detects global outliers:\n",
    "\n",
    "Randomly select a feature and split: The Isolation Forest algorithm starts by randomly selecting a feature from the dataset and a random split value within the range of that\n",
    "feature's values. This split divides the data into two parts: points that fall below the split value and points that fall above it.\n",
    "\n",
    "Recursively repeat the splitting process: The splitting process is repeated recursively for each resulting partition, creating a binary tree-like structure. At each step, \n",
    "a feature and split value are randomly chosen to further divide the data. The recursion continues until the points are completely isolated or a predefined depth limit is \n",
    "reached.\n",
    "\n",
    "Measure isolation: The isolation of a data point is determined by the average path length required to isolate it. The average path length is the average number of splits \n",
    "or branches needed to reach a data point in the constructed isolation tree.\n",
    "\n",
    "Calculate anomaly score: The anomaly score for each data point is computed based on its average path length. Points with shorter average path lengths are considered more\n",
    "likely to be outliers since they can be isolated more easily. The anomaly score is normalized to be within the range of [0, 1], with higher values indicating a higher\n",
    "likelihood of being a global outlier.\n",
    "\n",
    "Set a threshold: A threshold value can be set to classify data points as outliers or non-outliers based on their anomaly scores. Points with anomaly scores above the\n",
    "threshold are considered global outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ff45d-fd72-4231-89c7-80251a1ac003",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q11. What are some real-world applications where local outlier detection is more appropriate than global\n",
    "outlier detection, and vice versa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260233aa-7e3f-4fb5-8f31-63cec03861bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Local outlier detection and global outlier detection have different strengths and are suited for different types of real-world applications. Here are some scenarios where \n",
    "each approach may be more appropriate:\n",
    "\n",
    "Local Outlier Detection:\n",
    "\n",
    "Anomaly detection in sensor networks: In a sensor network, local outliers can indicate malfunctioning or abnormal behavior of individual sensors. Detecting local outliers \n",
    "helps in identifying specific sensors that are not functioning properly or providing inaccurate readings.\n",
    "\n",
    "Fraud detection in financial transactions: Local outliers can represent specific fraudulent transactions that deviate from the normal behavior within a localized context.\n",
    "Detecting local outliers helps in identifying individual transactions that are suspicious or fraudulent.\n",
    "\n",
    "Network intrusion detection: Local outliers can indicate anomalous behavior within a network, such as unusual network traffic patterns or specific activities that deviate\n",
    "from the norm. Detecting local outliers helps in identifying specific instances of network intrusion or malicious activity.\n",
    "\n",
    "Global Outlier Detection:\n",
    "\n",
    "Quality control in manufacturing: Global outliers can represent defective products or manufacturing processes that deviate significantly from the expected standard. \n",
    "Detecting global outliers helps in identifying overall issues in the manufacturing process and ensuring product quality.\n",
    "\n",
    "Anomaly detection in customer behavior: Global outliers can indicate uncommon or extreme behavior of customers that deviates from the overall customer behavior.\n",
    "Detecting global outliers helps in identifying unusual customer activities, such as high-value purchases or unusual browsing patterns.\n",
    "\n",
    "Environmental monitoring: Global outliers can indicate significant deviations in environmental variables, such as pollution levels or weather patterns, compared to the \n",
    "overall historical data. Detecting global outliers helps in identifying exceptional events or environmental changes that require attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92320ee-0c5e-4078-aaf5-c5af9a5b1709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d839da7-636d-4364-9271-933a6742b421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4ab38-5564-4076-a148-ba69aa8bc147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
